# Kaggle進階技巧 \(上\)

[理論講授 影片播放列表 \(YouTube\)](https://www.youtube.com/playlist?list=PL1f_B9coMEeDtAPmOMeqU4AQ70FywR1lP)

[今日課程 投影片下載 \(PDF\)](https://drive.google.com/file/d/1BdWY3e8vt3esk5Qn9yr6z_nJX6pn8hCI/view)

ch1概論

ch2預處理

ch3-1 數值特徵處理

ch3-2 類別特徵處理

ch3-3 其他進階特徵工程

## 1. Kaggle 概論

因為有競賽所有比較的標準，提供一個學習的平台

Why Kaggle - 最容易學習與應用 - 最具權威與知名度

* 程式碼共享：Kernel制度

* 豐富的資料集\(DataSet\)

* Google 收購並維護

* 參與者最廣泛

## 2. 預處理

#### 填補缺失值 - 什麼是缺失值

* 整條Column都缺失，直接刪除

* 整個Row都缺失，直接刪除

* 零星缺失值

#### 如何填補：

##### 數值特徵：

* 0：缺點是可能會混淆其他本來就是0的數值
* -999：用某個正常情況下不會出現的數值代替，但是選得不好可能會變成異常職
* Mean：平均數
* Median：中位數，跟平均數相比，不會被異常值干擾針對

##### 文字特徵

* Model：眾數，最常見的值
* 改成Others之類的值

##### 使用其他欄位輔助：可以依據常識來填值

#### 特徵縮放 - \[複習\]標準化 / 最大最小化

* 特徵縮放：平衡特徵間的影像力，如果不平痕，可能會影響到後續預測

* 標準化\(Standardization\)：計算 z-score，讓數據的mean為0、variance為1

* 最大最小化\(MinMaxScaler\)：把數據縮放到\[0,1\]或\[-1,1\]的區間

#### 特徵縮放 - 標準化 / 最大最小化 的使用時機

* 標準化：對SVM、Logistic regression或其他使用 squared loss funtion 的演算法來說，需要標準化
* 最大最小化：如果已經去除極端值或沒有極端值，建議使用最大最小化
* 至於Tree-Based的演算法，基本上都不需要標準化或最大最小化，因為對特徵縮放不敏感

## 3.特徵工程

### 3-1 數值特徵處理

##### 去除離群職\(Outlier\) - 為何要去離群職？

* 標準化或最大最小化都是現性轉換，都無法拉近離群值
* 不去除離群職就做標準化或最大最小化，會嚴重壓縮非離群職的資訊

#### 去除離群職\(Outlier\) - 如何去除離值

* 直接刪除：有時光是刪除離群值就有效果
* 限縮資料範圍：將離群值限制在範圍邊界上，如果資料非離群值欄位多，通常以保留盡可能多的資料

#### 去除偏態 - 為何要去除偏態？

* 當數值離群的比例相當大時，不能單純刪除離群值，必須全範圍調整
* 若不調整偏態，平均值失去代表性

#### 去除偏態 - 如何去除偏態

#### 常見的做法有log/sqrt/box-cox/rank

* Log\(取對數\)：處理偏態較大的資料
* Sqrt\(開根號\)：處理偏態較小的資料
* box-cox 分佈：使用 lambda參數混合前兩者去偏態方式\(lambda=0是log，lambda=0.5是sqrt\)
* rank：講資料依照大小順序重新編碼

##### 注意：log 與 box-cox 轉換數入值均不可為0，因此常用log1p取代log

#### 含雜訊數值資料的處理 - 裝箱法\(Binning\)/離散化

* 雜訊：量測變數中的隨機錯誤或偏差
* 處理方式：裝箱法：以區間標籤取代原數值，降低雜訊影響，數值型資料類別化，與原資料共存

#### 統計數值：

* 特殊數得計數：0的數量、NaN的數量、負數的數量
* 列統計值：min、max、median、skewness...等
* 列相似性：cosine similarity

#### \[進階\] 高斯對應\(Rank Gauss\)

* 欄位內容為名次/百分等級，改如何轉換為合理特徵？
* 作法 - 高斯對應：將名次/百分等級，對應到常態分佈累計機率值\(CDF\)的反函數
* 前提：假設實際分佈為常態分佈

3-2 類別特徵處理

* 標籤編碼\(Label Encoder\)：將欄位內不同類別，對應到相異整數
* 獨熱編碼\(One Hot Encoder\)：將欄位內不同類別，對應到相異欄位

標籤編碼 / 獨熱編碼 - 哪一個比較好？

* 標籤編碼：資料資訊密度高，計算快，樹狀模型使用標籤編碼，也會有不錯計算效果
* 獨熱編碼：資料每一欄位分開，數值的大小有意義，比較適合非樹狀模型的計算
* 綜合來說：獨熱編碼太過稀疏，記憶體與計算時間負荷重，預設採用標籤編碼
* 計數編碼\(Count Encoder\)：把單一類別的出現次數，視為特徵，注意：次數可能為零\(只出現在Test Set\)、非常態分佈，建議使用np.log1p\(\)去除偏態

```
train['cate_cnt'] = train['cate'].groupby(train['cate']).transform('count')
```

\[進階\] 平均值編碼\(Mean Encoding\)

* 依造類別目標值的平均值當作編碼，稱為平均值編碼\(例如：房產預測中，依造同一行政區的房產平均當作預估基準\)
* 前提：訓練集與測試集數據分佈相似

```
averages = train.groupby('cat')[target].agg(["mean"])
Train['cat'+'_avg'] = train['cat'+'_avg'].map(averages)
Test['cat'+'_avg'] = test['cat'+'_avg'].map(averages)
```

\[進階\] 平均值編碼的正規化

* 因為平均值編碼預測與訓練目標完全相同，非常容易overfitting，所以要做正規化
* 幾種正規化作法：1.使用平滑化\(Smoothing\)、2.使用K-Fold做Target Encoding3. 加入雜訊
* GatBoost：內建的文字行\(Categorical\)特徵就是用平均值編碼做



\[進階\] 嵌入式編碼\(Embedding Encoding\)

* 當單一欄位類別太多時，一般來說沒有很好的方式編碼，只能用雜揍編碼\(Feature Hash\)降低計算時間，但效果通常不好．
* 希望能將類別對應到n個數字，能抽取有意義特徵，又不像獨熱編碼稀疏
* 常見的情形分為兩類：1. 推薦系統、2.自然語言處理中的文字坎入\(Word Embedding\)



3-3 其他進階特徵工程

4.特徵選擇

4-1 特徵組合 4-2 特徵篩選

1. 模型驗證

2. 集成

\[附錄\] 補充資料

